{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mbart-large-cc25 finetuning\n",
    "Example notebook for ja-en finetuning based on [facebook/mbart-large-cc25](https://huggingface.co/facebook/mbart-large-cc25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install transformers[\"ja\"] numpy pandas sentencepiece fairseq\n",
    "!pip install -U jupyter ipywidgets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download training dataset\n",
    "今回はJESCのデータセットを利用します。  \n",
    "このような大規模なデータセットを公開していただいていることに感謝します。  \n",
    "This time we will use the [JESC dataset](https://nlp.stanford.edu/projects/jesc/index_ja.html) .  \n",
    "Thank you for publishing such a large dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget \"https://nlp.stanford.edu/projects/jesc/data/split.tar.gz\"\n",
    "!tar -zxvf split.tar.gz\n",
    "!ls split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training data for tokenizer\n",
    "Sentencepieceの学習に利用するデータを作成します。  \n",
    "Create the data used for learning the sentence piece."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for line in open('split/train', 'r', encoding='utf-8'):\n",
    "    text = line.split('\\t')\n",
    "    text = [t.rstrip('\\n') for t in text]\n",
    "    res.extend(text)\n",
    "for line in open('split/dev', 'r', encoding='utf-8'):\n",
    "    text = line.split('\\t')\n",
    "    text = [t.rstrip('\\n') for t in text]\n",
    "    res.extend(text)\n",
    "for line in open('split/test', 'r', encoding='utf-8'):\n",
    "    text = line.split('\\t')\n",
    "    text = [t.rstrip('\\n') for t in text]\n",
    "    res.extend(text)\n",
    "\n",
    "print(len(res))\n",
    "with open('tmp.txt', 'w') as f:\n",
    "    for d in res:\n",
    "        f.write(\"%s\\n\" % d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tail tmp.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training tokenizer\n",
    "Sentencepieceの学習をします。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# @NOTE\n",
    "# ボキャブラリーのサイズは適宜変更してください。\n",
    "# Please change the size of the vocabulary accordingly.\n",
    "spm.SentencePieceTrainer.Train(\"--input=tmp.txt --model_prefix=new_spm_model --vocab_size=64000 --vocabulary_output_piece_score=false --model_type=bpe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download pre-trained model\n",
    "後述の作業で必要になるので、huggingfaceではなくfairseqから直接事前学習済みモデルをダウンロードしてきます  \n",
    "Download the pre-trained model directly from fairseq instead of huggingface as you will need it for the tasks described below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget \"https://dl.fbaipublicfiles.com/fairseq/models/mbart/mbart.cc25.v2.tar.gz\"\n",
    "!tar -zxvf mbart.cc25.v2.tar.gz\n",
    "!ls mbart.cc25.v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight reduction of pre trained model\n",
    "ベースとなるモデル([facebook/mbart-large-cc25](https://huggingface.co/facebook/mbart-large-cc25))の軽量化をしていきます。  \n",
    "この工程は主にベースモデルのサイズが巨大でバッチサイズが1でも学習ができないという問題を解決するために実行します。  \n",
    "※[こちらのissue](https://github.com/pytorch/fairseq/issues/2120)で語られている問題です。  \n",
    "We will reduce the weight of the base model.  \n",
    "This process is mainly performed to solve the problem that the size of the base model is huge and even if the batch size is 1, it cannot be trained.  \n",
    "See this [issue](https://github.com/pytorch/fairseq/issues/2120) for more details.\n",
    "\n",
    "### 補足\n",
    "ここで実行する解決方法は、先ほど作成したボキャブラリーファイルをベースに、必要な単語の情報を残し不要な単語の情報を削除していくものです。  \n",
    "ベースとなっているモデルではおよそ25万の単語が収録されたボキャブラリーを使用していますが、ここには25種類の言語の単語が収録されています。  \n",
    "ファインチューニングでは目的となる言語を絞り込める(今回で言えば日本語と英語だけ)と思うので、必要な単語以外は切り落としてしまおうという手法です。  \n",
    "[こちらで提案されている](https://github.com/pytorch/fairseq/issues/2120#issuecomment-633460071)ものです。  \n",
    "以下のコードは主に[こちらのコメントのコード](https://github.com/pytorch/fairseq/issues/2120#issuecomment-647429120)がベースになっています。  \n",
    "素敵な手法を提案された[fansiawang氏](https://github.com/fansiawang)とサンプルコードをコメントしてくださった[ddaspit氏](https://github.com/ddaspit)に感謝申し上げます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting vocab\n",
    "先ほどつくったvocabファイルはそのままではこのあとの工程で使えないので加工します。  \n",
    "The vocab file created earlier cannot be used as it is in the subsequent process, so it will be processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "edited = []\n",
    "for line in open(\"new_spm_model.vocab\", 'r', encoding='utf-8'):\n",
    "    if line in [\"<unk>\\n\", \"<s>\\n\", \"</s>\\n\"]:\n",
    "        continue\n",
    "    new_line = line.rstrip('\\n') + \" 1\\n\"\n",
    "    edited.append(new_line)\n",
    "\n",
    "with open('new_dict.txt', 'w') as f:\n",
    "    for e in edited:\n",
    "        f.write(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce to create a new model.\n",
    "軽量化して新しいモデルを作成します。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir reduced_model\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairseq.data import Dictionary\n",
    "from transformers import (\n",
    "    MBartForConditionalGeneration, MBartTokenizer, MBartConfig\n",
    ")\n",
    "from typing import List\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs = [\n",
    "    \"ar_AR\",\n",
    "    \"cs_CZ\",\n",
    "    \"de_DE\",\n",
    "    \"en_XX\",\n",
    "    \"es_XX\",\n",
    "    \"et_EE\",\n",
    "    \"fi_FI\",\n",
    "    \"fr_XX\",\n",
    "    \"gu_IN\",\n",
    "    \"hi_IN\",\n",
    "    \"it_IT\",\n",
    "    \"ja_XX\",\n",
    "    \"kk_KZ\",\n",
    "    \"ko_KR\",\n",
    "    \"lt_LT\",\n",
    "    \"lv_LV\",\n",
    "    \"my_MM\",\n",
    "    \"ne_NP\",\n",
    "    \"nl_XX\",\n",
    "    \"ro_RO\",\n",
    "    \"ru_RU\",\n",
    "    \"si_LK\",\n",
    "    \"tr_TR\",\n",
    "    \"vi_VN\",\n",
    "    \"zh_CN\"\n",
    "]\n",
    "\n",
    "def load_dict(langs: List[str], path: str) -> Dictionary:\n",
    "    d = Dictionary.load(path)\n",
    "    for ll in langs:\n",
    "        d.add_symbol(f\"[{ll}]\")\n",
    "    d.add_symbol(\"<mask>\")\n",
    "    d.add_symbol(\"<pad>\")\n",
    "    return d\n",
    "\n",
    "\n",
    "pre_dict = load_dict(langs, \"./mbart.cc25.v2/dict.txt\")\n",
    "ft_dict = load_dict(langs, \"./new_dict.txt\")\n",
    "\n",
    "model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-cc25\")\n",
    "org_sd = model.state_dict()\n",
    "resized_sd = model.state_dict()\n",
    "\n",
    "mapping: List[int] = []\n",
    "for i in range(len(ft_dict)):\n",
    "    word = ft_dict[i]\n",
    "    mapping.append(pre_dict.index(word))\n",
    "\n",
    "for name in [\"model.encoder.embed_tokens.weight\", \"model.decoder.embed_tokens.weight\", \"model.shared.weight\", \"lm_head.weight\"]:\n",
    "    pre_tensor: torch.Tensor = org_sd[name]\n",
    "    ft_tensor = torch.zeros(\n",
    "        [len(ft_dict), 1024], dtype=pre_tensor.dtype, layout=pre_tensor.layout, device=pre_tensor.device,\n",
    "    )\n",
    "    for ft_i, pre_i in enumerate(mapping):\n",
    "        ft_tensor[ft_i] = pre_tensor[pre_i]\n",
    "    resized_sd[name] = ft_tensor\n",
    "resized_sd[\"final_logits_bias\"] = resized_sd[\"final_logits_bias\"][:, :len(ft_dict)]\n",
    "\n",
    "config = MBartConfig.from_pretrained(\"facebook/mbart-large-cc25\")\n",
    "config.vocab_size = len(ft_dict)\n",
    "print(config)\n",
    "new_model = MBartForConditionalGeneration.from_pretrained(None, config=config, state_dict=resized_sd)\n",
    "new_model.save_pretrained(\"./reduced_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls reduced_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上でベースモデルの軽量化が完了します。  \n",
    "ここからは `reduced_model` ディレクトリをpre-trainedモデルとして利用していきます。  \n",
    "This completes the weight reduction of the base model.  \n",
    "From now on, we will use the `reduced_model` directory as a pre-trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation of Tokenizer\n",
    "今のままでは不足しているファイルがあるので取得します  \n",
    "Get the missing files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = MBartTokenizer.from_pretrained(\"facebook/mbart-large-cc25\")\n",
    "tokenizer.save_pretrained(\"./reduced_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルファイルは先ほど作成したもので上書きします  \n",
    "Overwrite the model file with the one you created earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv ./new_spm_model.model ./reduced_model/sentencepiece.bpe.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -al ./reduced_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上でモデルとトークナイザー両方が `reduced_model` ディレクトリから呼べるようになります  \n",
    "Now both the model and the tokenizer can be called from the `reduced_model` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MBartForConditionalGeneration.from_pretrained(\"./reduced_model\")\n",
    "tokenizer = MBartTokenizer.from_pretrained(\"./reduced_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "モデルとトークナイザーの準備ができたので、トレーニングを実行します。  \n",
    "トレーニングのコードは[こちら](https://www.kaggle.com/ajax0564/mbart-finetuning-hintoenglish-translation)を参考にしています  \n",
    "Now that the model and tokenizer are ready, it's time to start training.  \n",
    "[Here](https://www.kaggle.com/ajax0564/mbart-finetuning-hintoenglish-translation) is the reference code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    ")\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "result_dir = \"./output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_collator(features: list):\n",
    "    x = [f[\"translation\"][\"ja\"] for f in features]\n",
    "    y = [f[\"translation\"][\"en\"] for f in features]\n",
    "    inputs = tokenizer(x, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=32)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        inputs['labels'] = tokenizer(y, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=48)['input_ids']\n",
    "    return inputs\n",
    "\n",
    "tokenizer = MBartTokenizer.from_pretrained(\"./reduced_model\", src_lang=\"ja_XX\", tgt_lang=\"en_XX\")\n",
    "tokenizer.save_pretrained(result_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "eval_data = []\n",
    "\n",
    "for line in open(\"./split/train\", \"r\", encoding='utf-8'):\n",
    "    text = line.split('\\t')\n",
    "    train_data.append(\n",
    "        {\"translation\": {\n",
    "            \"ja\": text[1].rstrip('\\n'),\n",
    "            \"en\": text[0].rstrip('\\n')\n",
    "        }}\n",
    "    )\n",
    "print(f\"train_data size: {len(train_data)}\")\n",
    "\n",
    "for line in open(\"./split/dev\", \"r\", encoding='utf-8'):\n",
    "    text = line.split('\\t')\n",
    "    eval_data.append(\n",
    "        {\"translation\": {\n",
    "            \"ja\": text[1].rstrip('\\n'),\n",
    "            \"en\": text[0].rstrip('\\n')\n",
    "        }}\n",
    "    )\n",
    "print(f\"eval_data size: {len(eval_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 1\n",
    "learning_rate = 3e-5\n",
    "epochs = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MBartForConditionalGeneration.from_pretrained(\"./reduced_model\")\n",
    "\n",
    "args = Seq2SeqTrainingArguments(output_dir=result_dir,\n",
    "                                do_train=True,\n",
    "                                do_eval=True,\n",
    "                                per_device_train_batch_size=batch_size,\n",
    "                                per_device_eval_batch_size=batch_size,\n",
    "                                learning_rate=learning_rate,\n",
    "                                num_train_epochs=epochs,\n",
    "                                evaluation_strategy=\"epoch\",\n",
    "                                )\n",
    "\n",
    "trainer = Seq2SeqTrainer(model=model,\n",
    "                         args=args,\n",
    "                         data_collator=data_collator,\n",
    "                         train_dataset=train_data,\n",
    "                         eval_dataset=eval_data,\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "trainer.save_model(result_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "できあがったモデルを使って推論を実行してみます  \n",
    "Let's perform inference using the resulting model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MBartForConditionalGeneration.from_pretrained(\"./output\")\n",
    "tokenizer = MBartTokenizer.from_pretrained(\"./output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"おはよう\"\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "translated_tokens = model.generate(**inputs, decoder_start_token_id=tokenizer.lang_code_to_id[\"en_XX\"], early_stopping=True, max_length=48)\n",
    "pred = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n",
    "print(f\"日本語 - {sentence}: English - {pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Base Python)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/python-3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
