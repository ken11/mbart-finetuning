{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mbart-large-cc25 finetuning\n",
    "Example notebook for ja-en finetuning based on [facebook/mbart-large-cc25](https://huggingface.co/facebook/mbart-large-cc25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Collecting torch==1.9.0+cu111\n",
      "  Using cached https://download.pytorch.org/whl/cu111/torch-1.9.0%2Bcu111-cp36-cp36m-linux_x86_64.whl (2041.3 MB)\n",
      "Collecting torchvision==0.10.0+cu111\n",
      "  Using cached https://download.pytorch.org/whl/cu111/torchvision-0.10.0%2Bcu111-cp36-cp36m-linux_x86_64.whl (23.2 MB)\n",
      "Collecting torchaudio==0.9.0\n",
      "  Using cached torchaudio-0.9.0-cp36-cp36m-manylinux1_x86_64.whl (1.9 MB)\n",
      "Collecting typing-extensions\n",
      "  Using cached typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\n",
      "Collecting dataclasses\n",
      "  Using cached dataclasses-0.8-py3-none-any.whl (19 kB)\n",
      "Collecting numpy\n",
      "  Using cached numpy-1.19.5-cp36-cp36m-manylinux2010_x86_64.whl (14.8 MB)\n",
      "Collecting pillow>=5.3.0\n",
      "  Using cached Pillow-8.3.2-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "Installing collected packages: typing-extensions, dataclasses, torch, pillow, numpy, torchvision, torchaudio\n",
      "Successfully installed dataclasses-0.8 numpy-1.19.5 pillow-8.3.2 torch-1.9.0+cu111 torchaudio-0.9.0 torchvision-0.10.0+cu111 typing-extensions-3.10.0.2\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.3 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.6/site-packages (21.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: transformers[ja] in /usr/local/lib/python3.6/site-packages (4.11.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/site-packages (1.19.5)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.6/site-packages (1.1.5)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/site-packages (0.1.96)\n",
      "Collecting fairseq\n",
      "  Downloading fairseq-0.10.2-cp36-cp36m-manylinux1_x86_64.whl (1.7 MB)\n",
      "     |████████████████████████████████| 1.7 MB 9.5 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/site-packages (from transformers[ja]) (4.62.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.6/site-packages (from transformers[ja]) (21.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/site-packages (from transformers[ja]) (2.26.0)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/site-packages (from transformers[ja]) (0.0.46)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.6/site-packages (from transformers[ja]) (0.10.3)\n",
      "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/site-packages (from transformers[ja]) (0.8)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/site-packages (from transformers[ja]) (2021.10.8)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/site-packages (from transformers[ja]) (3.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.0.17 in /usr/local/lib/python3.6/site-packages (from transformers[ja]) (0.0.19)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.6/site-packages (from transformers[ja]) (4.8.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.6/site-packages (from transformers[ja]) (5.4.1)\n",
      "Requirement already satisfied: fugashi>=1.0 in /usr/local/lib/python3.6/site-packages (from transformers[ja]) (1.1.1)\n",
      "Requirement already satisfied: unidic>=1.0.2 in /usr/local/lib/python3.6/site-packages (from transformers[ja]) (1.1.0)\n",
      "Requirement already satisfied: unidic-lite>=1.0.7 in /usr/local/lib/python3.6/site-packages (from transformers[ja]) (1.0.8)\n",
      "Requirement already satisfied: ipadic<2.0,>=1.0.0 in /usr/local/lib/python3.6/site-packages (from transformers[ja]) (1.0.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/site-packages (from pandas) (2.8.2)\n",
      "Collecting hydra-core\n",
      "  Downloading hydra_core-1.1.1-py3-none-any.whl (145 kB)\n",
      "     |████████████████████████████████| 145 kB 109.3 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: cffi in /usr/local/lib/python3.6/site-packages (from fairseq) (1.14.6)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.6/site-packages (from fairseq) (1.9.0+cu111)\n",
      "Collecting sacrebleu>=1.4.12\n",
      "  Downloading sacrebleu-2.0.0-py3-none-any.whl (90 kB)\n",
      "     |████████████████████████████████| 90 kB 1.8 MB/s              \n",
      "\u001b[?25hCollecting cython\n",
      "  Downloading Cython-0.29.24-cp36-cp36m-manylinux1_x86_64.whl (2.0 MB)\n",
      "     |████████████████████████████████| 2.0 MB 70.3 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.6/site-packages (from huggingface-hub>=0.0.17->transformers[ja]) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/site-packages (from packaging>=20.0->transformers[ja]) (2.4.7)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in /usr/local/lib/python3.6/site-packages (from sacrebleu>=1.4.12->fairseq) (0.4.3)\n",
      "Collecting tabulate>=0.8.9\n",
      "  Downloading tabulate-0.8.9-py3-none-any.whl (25 kB)\n",
      "Collecting portalocker\n",
      "  Downloading portalocker-2.3.2-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: plac<2.0.0,>=1.1.3 in /usr/local/lib/python3.6/site-packages (from unidic>=1.0.2->transformers[ja]) (1.3.3)\n",
      "Requirement already satisfied: wasabi<1.0.0,>=0.6.0 in /usr/local/lib/python3.6/site-packages (from unidic>=1.0.2->transformers[ja]) (0.8.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/site-packages (from requests->transformers[ja]) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.6/site-packages (from requests->transformers[ja]) (2.0.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.6/site-packages (from requests->transformers[ja]) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.6/site-packages (from requests->transformers[ja]) (1.26.7)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/site-packages (from cffi->fairseq) (2.20)\n",
      "Collecting antlr4-python3-runtime==4.8\n",
      "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
      "     |████████████████████████████████| 112 kB 101.9 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting importlib-resources\n",
      "  Downloading importlib_resources-5.2.2-py3-none-any.whl (27 kB)\n",
      "Collecting omegaconf==2.1.*\n",
      "  Downloading omegaconf-2.1.1-py3-none-any.whl (74 kB)\n",
      "     |████████████████████████████████| 74 kB 438 kB/s             \n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/site-packages (from importlib-metadata->transformers[ja]) (3.6.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/site-packages (from sacremoses->transformers[ja]) (8.0.3)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/site-packages (from sacremoses->transformers[ja]) (1.1.0)\n",
      "Building wheels for collected packages: antlr4-python3-runtime\n",
      "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141230 sha256=a8ad19a8f0e82cb59bd7c859a476aeee913e4c99dd9e363774efea8a13a21a97\n",
      "  Stored in directory: /root/.cache/pip/wheels/a8/04/35/9449686f1c26ff16f6224dc942e108329f3782185802ec6b93\n",
      "Successfully built antlr4-python3-runtime\n",
      "Installing collected packages: antlr4-python3-runtime, tabulate, portalocker, omegaconf, importlib-resources, sacrebleu, hydra-core, cython, fairseq\n",
      "Successfully installed antlr4-python3-runtime-4.8 cython-0.29.24 fairseq-0.10.2 hydra-core-1.1.1 importlib-resources-5.2.2 omegaconf-2.1.1 portalocker-2.3.2 sacrebleu-2.0.0 tabulate-0.8.9\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: jupyter in /usr/local/lib/python3.6/site-packages (1.0.0)\n",
      "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.6/site-packages (7.6.5)\n",
      "Requirement already satisfied: notebook in /usr/local/lib/python3.6/site-packages (from jupyter) (6.4.4)\n",
      "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/site-packages (from jupyter) (5.5.5)\n",
      "Requirement already satisfied: qtconsole in /usr/local/lib/python3.6/site-packages (from jupyter) (5.1.1)\n",
      "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/site-packages (from jupyter) (6.0.7)\n",
      "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.6/site-packages (from jupyter) (6.4.0)\n",
      "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.6/site-packages (from ipywidgets) (7.16.1)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.6/site-packages (from ipywidgets) (3.5.1)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.6/site-packages (from ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.6/site-packages (from ipywidgets) (1.0.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.6/site-packages (from ipywidgets) (4.3.3)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.6/site-packages (from ipywidgets) (5.1.3)\n",
      "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/site-packages (from ipykernel->jupyter) (7.0.3)\n",
      "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.6/site-packages (from ipykernel->jupyter) (6.1)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.6/site-packages (from ipython>=4.0.0->ipywidgets) (5.1.0)\n",
      "Requirement already satisfied: jedi>=0.10 in /usr/local/lib/python3.6/site-packages (from ipython>=4.0.0->ipywidgets) (0.18.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/site-packages (from ipython>=4.0.0->ipywidgets) (53.0.0)\n",
      "Requirement already satisfied: pexpect in /usr/local/lib/python3.6/site-packages (from ipython>=4.0.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.6/site-packages (from ipython>=4.0.0->ipywidgets) (3.0.20)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.6/site-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/site-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.6/site-packages (from ipython>=4.0.0->ipywidgets) (2.10.0)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/site-packages (from nbformat>=4.2.0->ipywidgets) (3.2.0)\n",
      "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/site-packages (from nbformat>=4.2.0->ipywidgets) (4.8.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/site-packages (from traitlets>=4.3.1->ipywidgets) (1.16.0)\n",
      "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.6/site-packages (from notebook->jupyter) (0.11.0)\n",
      "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.6/site-packages (from notebook->jupyter) (21.1.0)\n",
      "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.6/site-packages (from notebook->jupyter) (19.0.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.6/site-packages (from notebook->jupyter) (0.12.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/site-packages (from notebook->jupyter) (3.0.2)\n",
      "Requirement already satisfied: Send2Trash>=1.5.0 in /usr/local/lib/python3.6/site-packages (from notebook->jupyter) (1.8.0)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/site-packages (from nbconvert->jupyter) (0.8.4)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/site-packages (from nbconvert->jupyter) (1.5.0)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /usr/local/lib/python3.6/site-packages (from nbconvert->jupyter) (0.5.4)\n",
      "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/site-packages (from nbconvert->jupyter) (0.7.1)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/site-packages (from nbconvert->jupyter) (0.3)\n",
      "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.6/site-packages (from nbconvert->jupyter) (0.1.2)\n",
      "Requirement already satisfied: testpath in /usr/local/lib/python3.6/site-packages (from nbconvert->jupyter) (0.5.0)\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python3.6/site-packages (from nbconvert->jupyter) (4.1.0)\n",
      "Requirement already satisfied: qtpy in /usr/local/lib/python3.6/site-packages (from qtconsole->jupyter) (1.11.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.6/site-packages (from jedi>=0.10->ipython>=4.0.0->ipywidgets) (0.8.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.6/site-packages (from jinja2->notebook->jupyter) (2.0.1)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.6/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (4.8.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (21.2.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.6/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (0.18.0)\n",
      "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.6/site-packages (from jupyter-client->ipykernel->jupyter) (1.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/site-packages (from jupyter-client->ipykernel->jupyter) (2.8.2)\n",
      "Requirement already satisfied: async-generator in /usr/local/lib/python3.6/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->jupyter) (1.10)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.6/site-packages (from terminado>=0.8.3->notebook->jupyter) (0.7.0)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /usr/local/lib/python3.6/site-packages (from argon2-cffi->notebook->jupyter) (1.14.6)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/site-packages (from bleach->nbconvert->jupyter) (0.5.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/site-packages (from bleach->nbconvert->jupyter) (21.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/site-packages (from cffi>=1.0.0->argon2-cffi->notebook->jupyter) (2.20)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/site-packages (from importlib-metadata->jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.6/site-packages (from importlib-metadata->jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/site-packages (from packaging->bleach->nbconvert->jupyter) (2.4.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install transformers[\"ja\"] numpy pandas sentencepiece fairseq\n",
    "!pip install -U jupyter ipywidgets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download training dataset\n",
    "今回はJESCのデータセットを利用します。  \n",
    "このような大規模なデータセットを公開していただいていることに感謝します。  \n",
    "This time we will use the [JESC dataset](https://nlp.stanford.edu/projects/jesc/index_ja.html) .  \n",
    "Thank you for publishing such a large dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev  test  train\n"
     ]
    }
   ],
   "source": [
    "!wget \"https://nlp.stanford.edu/projects/jesc/data/split.tar.gz\"\n",
    "!tar -zxvf split.tar.gz\n",
    "!ls split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training data for tokenizer\n",
    "Sentencepieceの学習に利用するデータを作成します。  \n",
    "Create the data used for learning the sentence piece."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5602776\n"
     ]
    }
   ],
   "source": [
    "res = []\n",
    "for line in open('split/train', 'r', encoding='utf-8'):\n",
    "    text = line.split('\\t')\n",
    "    text = [t.rstrip('\\n') for t in text]\n",
    "    res.extend(text)\n",
    "for line in open('split/dev', 'r', encoding='utf-8'):\n",
    "    text = line.split('\\t')\n",
    "    text = [t.rstrip('\\n') for t in text]\n",
    "    res.extend(text)\n",
    "for line in open('split/test', 'r', encoding='utf-8'):\n",
    "    text = line.split('\\t')\n",
    "    text = [t.rstrip('\\n') for t in text]\n",
    "    res.extend(text)\n",
    "\n",
    "print(len(res))\n",
    "with open('tmp.txt', 'w') as f:\n",
    "    for d in res:\n",
    "        f.write(\"%s\\n\" % d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "am i free to go?\n",
      "行っていいのですか?\n",
      "you'll definitely become a good nurse.\n",
      "あんた きっと いい看護師になるよ。\n",
      "and isn't that what your facemash was about?\n",
      "フェイスマッシュも同じだ\n",
      "what is it this time?\n",
      "ロケットパンチ止めようとしてんぞ! どういう状況だ? これ!\n",
      "you poured your father's remains in and closed it.\n",
      "一度 封をはがして灰を入れ また封をしても―\n"
     ]
    }
   ],
   "source": [
    "!tail tmp.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training tokenizer\n",
    "Sentencepieceの学習をします。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaa\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# @NOTE\n",
    "# ボキャブラリーのサイズは適宜変更してください。\n",
    "# Please change the size of the vocabulary accordingly.\n",
    "spm.SentencePieceTrainer.Train(\"--input=tmp.txt --model_prefix=new_spm_model --vocab_size=64000 --vocabulary_output_piece_score=false --model_type=bpe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download pre-trained model\n",
    "後述の作業で必要になるので、huggingfaceではなくfairseqから直接事前学習済みモデルをダウンロードしてきます  \n",
    "Download the pre-trained model directly from fairseq instead of huggingface as you will need it for the tasks described below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-10-13 12:16:27--  https://dl.fbaipublicfiles.com/fairseq/models/mbart/mbart.cc25.v2.tar.gz\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 172.67.9.4, 104.22.75.142, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5618826847 (5.2G) [application/gzip]\n",
      "Saving to: ‘mbart.cc25.v2.tar.gz’\n",
      "\n",
      "mbart.cc25.v2.tar.g 100%[===================>]   5.23G  52.5MB/s    in 2m 1s   \n",
      "\n",
      "2021-10-13 12:18:39 (44.4 MB/s) - ‘mbart.cc25.v2.tar.gz’ saved [5618826847/5618826847]\n",
      "\n",
      "mbart.cc25.v2/\n",
      "mbart.cc25.v2/sentence.bpe.model\n",
      "tar: mbart.cc25.v2/sentence.bpe.model: Cannot change ownership to uid 1185301073, gid 1185301073: Invalid argument\n",
      "mbart.cc25.v2/dict.txt\n",
      "tar: mbart.cc25.v2/dict.txt: Cannot change ownership to uid 1185301073, gid 1185301073: Invalid argument\n",
      "mbart.cc25.v2/model.pt\n",
      "tar: mbart.cc25.v2/model.pt: Cannot change ownership to uid 1185301073, gid 1185301073: Invalid argument\n",
      "tar: mbart.cc25.v2: Cannot change ownership to uid 1185301073, gid 1185301073: Invalid argument\n",
      "tar: Exiting with failure status due to previous errors\n",
      "dict.txt  model.pt  sentence.bpe.model\n"
     ]
    }
   ],
   "source": [
    "!wget \"https://dl.fbaipublicfiles.com/fairseq/models/mbart/mbart.cc25.v2.tar.gz\"\n",
    "!tar -zxvf mbart.cc25.v2.tar.gz\n",
    "!ls mbart.cc25.v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight reduction of pre trained model\n",
    "ベースとなるモデル([facebook/mbart-large-cc25](https://huggingface.co/facebook/mbart-large-cc25))の軽量化をしていきます。  \n",
    "この工程は主にベースモデルのサイズが巨大でバッチサイズが1でも学習ができないという問題を解決するために実行します。  \n",
    "※[こちらのissue](https://github.com/pytorch/fairseq/issues/2120)で語られている問題です。  \n",
    "We will reduce the weight of the base model.  \n",
    "This process is mainly performed to solve the problem that the size of the base model is huge and even if the batch size is 1, it cannot be trained.  \n",
    "See this [issue](https://github.com/pytorch/fairseq/issues/2120) for more details.\n",
    "\n",
    "### 補足\n",
    "ここで実行する解決方法は、先ほど作成したボキャブラリーファイルをベースに、必要な単語の情報を残し不要な単語の情報を削除していくものです。  \n",
    "ベースとなっているモデルではおよそ25万の単語が収録されたボキャブラリーを使用していますが、ここには25種類の言語の単語が収録されています。  \n",
    "ファインチューニングでは目的となる言語を絞り込める(今回で言えば日本語と英語だけ)と思うので、必要な単語以外は切り落としてしまおうという手法です。  \n",
    "[こちらで提案されている](https://github.com/pytorch/fairseq/issues/2120#issuecomment-633460071)ものです。  \n",
    "以下のコードは主に[こちらのコメントのコード](https://github.com/pytorch/fairseq/issues/2120#issuecomment-647429120)がベースになっています。  \n",
    "素敵な手法を提案された[fansiawang氏](https://github.com/fansiawang)とサンプルコードをコメントしてくださった[ddaspit氏](https://github.com/ddaspit)に感謝申し上げます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting vocab\n",
    "先ほどつくったvocabファイルはそのままではこのあとの工程で使えないので加工します。  \n",
    "The vocab file created earlier cannot be used as it is in the subsequent process, so it will be processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "edited = []\n",
    "for line in open(\"new_spm_model.vocab\", 'r', encoding='utf-8'):\n",
    "    if line in [\"<unk>\\n\", \"<s>\\n\", \"</s>\\n\"]:\n",
    "        continue\n",
    "    new_line = line.rstrip('\\n') + \" 1\\n\"\n",
    "    edited.append(new_line)\n",
    "\n",
    "with open('new_dict.txt', 'w') as f:\n",
    "    for e in edited:\n",
    "        f.write(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untitled.ipynb\tnew_spm_model.model  split\t   tmp.txt\n",
      "new_dict.txt\tnew_spm_model.vocab  split.tar.gz  translation.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce to create a new model.\n",
    "軽量化して新しいモデルを作成します。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untitled.ipynb\t      new_dict.txt\t   reduced_model  tmp.txt\n",
      "mbart.cc25.v2\t      new_spm_model.model  split\t  translation.ipynb\n",
      "mbart.cc25.v2.tar.gz  new_spm_model.vocab  split.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!mkdir reduced_model\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairseq.data import Dictionary\n",
    "from transformers import (\n",
    "    MBartForConditionalGeneration, MBartTokenizer, MBartConfig\n",
    ")\n",
    "from typing import List\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MBartConfig {\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": true,\n",
      "  \"architectures\": [\n",
      "    \"MBartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 1024,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"mbart\",\n",
      "  \"normalize_before\": true,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 5,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"decoder_start_token_id\": 250020\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64027\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "langs = [\n",
    "    \"ar_AR\",\n",
    "    \"cs_CZ\",\n",
    "    \"de_DE\",\n",
    "    \"en_XX\",\n",
    "    \"es_XX\",\n",
    "    \"et_EE\",\n",
    "    \"fi_FI\",\n",
    "    \"fr_XX\",\n",
    "    \"gu_IN\",\n",
    "    \"hi_IN\",\n",
    "    \"it_IT\",\n",
    "    \"ja_XX\",\n",
    "    \"kk_KZ\",\n",
    "    \"ko_KR\",\n",
    "    \"lt_LT\",\n",
    "    \"lv_LV\",\n",
    "    \"my_MM\",\n",
    "    \"ne_NP\",\n",
    "    \"nl_XX\",\n",
    "    \"ro_RO\",\n",
    "    \"ru_RU\",\n",
    "    \"si_LK\",\n",
    "    \"tr_TR\",\n",
    "    \"vi_VN\",\n",
    "    \"zh_CN\"\n",
    "]\n",
    "\n",
    "def load_dict(langs: List[str], path: str) -> Dictionary:\n",
    "    d = Dictionary.load(path)\n",
    "    for ll in langs:\n",
    "        d.add_symbol(f\"[{ll}]\")\n",
    "    d.add_symbol(\"<mask>\")\n",
    "    d.add_symbol(\"<pad>\")\n",
    "    return d\n",
    "\n",
    "\n",
    "pre_dict = load_dict(langs, \"./mbart.cc25.v2/dict.txt\")\n",
    "ft_dict = load_dict(langs, \"./new_dict.txt\")\n",
    "\n",
    "model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-cc25\")\n",
    "org_sd = model.state_dict()\n",
    "resized_sd = model.state_dict()\n",
    "\n",
    "mapping: List[int] = []\n",
    "for i in range(len(ft_dict)):\n",
    "    word = ft_dict[i]\n",
    "    mapping.append(pre_dict.index(word))\n",
    "\n",
    "for name in [\"model.encoder.embed_tokens.weight\", \"model.decoder.embed_tokens.weight\", \"model.shared.weight\", \"lm_head.weight\"]:\n",
    "    pre_tensor: torch.Tensor = org_sd[name]\n",
    "    ft_tensor = torch.zeros(\n",
    "        [len(ft_dict), 1024], dtype=pre_tensor.dtype, layout=pre_tensor.layout, device=pre_tensor.device,\n",
    "    )\n",
    "    for ft_i, pre_i in enumerate(mapping):\n",
    "        ft_tensor[ft_i] = pre_tensor[pre_i]\n",
    "    resized_sd[name] = ft_tensor\n",
    "resized_sd[\"final_logits_bias\"] = resized_sd[\"final_logits_bias\"][:, :len(ft_dict)]\n",
    "\n",
    "config = MBartConfig.from_pretrained(\"facebook/mbart-large-cc25\")\n",
    "config.vocab_size = len(ft_dict)\n",
    "print(config)\n",
    "new_model = MBartForConditionalGeneration.from_pretrained(None, config=config, state_dict=resized_sd)\n",
    "new_model.save_pretrained(\"./reduced_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json  pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "!ls reduced_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上でベースモデルの軽量化が完了します。  \n",
    "ここからは `reduced_model` ディレクトリをpre-trainedモデルとして利用していきます。  \n",
    "This completes the weight reduction of the base model.  \n",
    "From now on, we will use the `reduced_model` directory as a pre-trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation of Tokenizer\n",
    "今のままでは不足しているファイルがあるので取得します  \n",
    "Get the missing files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./reduced_model/tokenizer_config.json',\n",
       " './reduced_model/special_tokens_map.json',\n",
       " './reduced_model/sentencepiece.bpe.model',\n",
       " './reduced_model/added_tokens.json')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = MBartTokenizer.from_pretrained(\"facebook/mbart-large-cc25\")\n",
    "tokenizer.save_pretrained(\"./reduced_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルファイルは先ほど作成したもので上書きします  \n",
    "Overwrite the model file with the one you created earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv ./new_spm_model.model ./reduced_model/sentencepiece.bpe.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1643992\n",
      "drwxr-xr-x 3 root root       6144 Oct 13 12:49 .\n",
      "drwxr-xr-x 6 root root       6144 Oct 13 12:49 ..\n",
      "drwxr-xr-x 2 root root       6144 Oct 13 12:47 .ipynb_checkpoints\n",
      "-rw-r--r-- 1 root root       1364 Oct 13 12:36 config.json\n",
      "-rw-r--r-- 1 root root 1681991993 Oct 13 12:36 pytorch_model.bin\n",
      "-rw-r--r-- 1 root root    1427810 Oct 13 11:31 sentencepiece.bpe.model\n",
      "-rw-r--r-- 1 root root        494 Oct 13 12:46 special_tokens_map.json\n",
      "-rw-r--r-- 1 root root        686 Oct 13 12:46 tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "!ls -al ./reduced_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上でモデルとトークナイザー両方が `reduced_model` ディレクトリから呼べるようになります  \n",
    "Now both the model and the tokenizer can be called from the `reduced_model` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MBartForConditionalGeneration.from_pretrained(\"./reduced_model\")\n",
    "tokenizer = MBartTokenizer.from_pretrained(\"./reduced_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "モデルとトークナイザーの準備ができたので、トレーニングを実行します。  \n",
    "トレーニングのコードは[こちら](https://www.kaggle.com/ajax0564/mbart-finetuning-hintoenglish-translation)を参考にしています  \n",
    "Now that the model and tokenizer are ready, it's time to start training.  \n",
    "[Here](https://www.kaggle.com/ajax0564/mbart-finetuning-hintoenglish-translation) is the reference code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    ")\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "result_dir = \"./output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./output/tokenizer_config.json',\n",
       " './output/special_tokens_map.json',\n",
       " './output/sentencepiece.bpe.model',\n",
       " './output/added_tokens.json')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def data_collator(features: list):\n",
    "    x = [f[\"translation\"][\"ja\"] for f in features]\n",
    "    y = [f[\"translation\"][\"en\"] for f in features]\n",
    "    inputs = tokenizer(x, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=32)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        inputs['labels'] = tokenizer(y, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=48)['input_ids']\n",
    "    return inputs\n",
    "\n",
    "tokenizer = MBartTokenizer.from_pretrained(\"./reduced_model\", src_lang=\"ja_XX\", tgt_lang=\"en_XX\")\n",
    "tokenizer.save_pretrained(result_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data size: 2000\n",
      "eval_data size: 2000\n"
     ]
    }
   ],
   "source": [
    "train_data = []\n",
    "eval_data = []\n",
    "\n",
    "for line in open(\"./split/train\", \"r\", encoding='utf-8'):\n",
    "    text = line.split('\\t')\n",
    "    train_data.append(\n",
    "        {\"translation\": {\n",
    "            \"ja\": text[1].rstrip('\\n'),\n",
    "            \"en\": text[0].rstrip('\\n')\n",
    "        }}\n",
    "    )\n",
    "print(f\"train_data size: {len(train_data)}\")\n",
    "\n",
    "for line in open(\"./split/dev\", \"r\", encoding='utf-8'):\n",
    "    text = line.split('\\t')\n",
    "    eval_data.append(\n",
    "        {\"translation\": {\n",
    "            \"ja\": text[1].rstrip('\\n'),\n",
    "            \"en\": text[0].rstrip('\\n')\n",
    "        }}\n",
    "    )\n",
    "print(f\"eval_data size: {len(eval_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 1\n",
    "learning_rate = 3e-5\n",
    "epochs = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MBartForConditionalGeneration.from_pretrained(\"./reduced_model\")\n",
    "\n",
    "args = Seq2SeqTrainingArguments(output_dir=result_dir,\n",
    "                                do_train=True,\n",
    "                                do_eval=True,\n",
    "                                per_device_train_batch_size=batch_size,\n",
    "                                per_device_eval_batch_size=batch_size,\n",
    "                                learning_rate=learning_rate,\n",
    "                                num_train_epochs=epochs,\n",
    "                                evaluation_strategy=\"epoch\",\n",
    "                                )\n",
    "\n",
    "trainer = Seq2SeqTrainer(model=model,\n",
    "                         args=args,\n",
    "                         data_collator=data_collator,\n",
    "                         train_dataset=train_data,\n",
    "                         eval_dataset=eval_data,\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 2000\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2000/2000 13:02, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.045500</td>\n",
       "      <td>0.961348</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/checkpoint-500\n",
      "Configuration saved in ./output/checkpoint-500/config.json\n",
      "Model weights saved in ./output/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to ./output/checkpoint-1000\n",
      "Configuration saved in ./output/checkpoint-1000/config.json\n",
      "Model weights saved in ./output/checkpoint-1000/pytorch_model.bin\n",
      "Saving model checkpoint to ./output/checkpoint-1500\n",
      "Configuration saved in ./output/checkpoint-1500/config.json\n",
      "Model weights saved in ./output/checkpoint-1500/pytorch_model.bin\n",
      "Saving model checkpoint to ./output/checkpoint-2000\n",
      "Configuration saved in ./output/checkpoint-2000/config.json\n",
      "Model weights saved in ./output/checkpoint-2000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to ./output\n",
      "Configuration saved in ./output/config.json\n",
      "Model weights saved in ./output/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.save_model(result_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "できあがったモデルを使って推論を実行してみます  \n",
    "Let's perform inference using the resulting model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./output/config.json\n",
      "Model config MBartConfig {\n",
      "  \"_name_or_path\": \"./reduced_model\",\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": true,\n",
      "  \"architectures\": [\n",
      "    \"MBartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 1024,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"mbart\",\n",
      "  \"normalize_before\": true,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 5,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"decoder_start_token_id\": 250020\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64027\n",
      "}\n",
      "\n",
      "loading weights file ./output/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MBartForConditionalGeneration.\n",
      "\n",
      "All the weights of MBartForConditionalGeneration were initialized from the model checkpoint at ./output.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MBartForConditionalGeneration for predictions without further training.\n",
      "Didn't find file ./output/added_tokens.json. We won't load it.\n",
      "Didn't find file ./output/tokenizer.json. We won't load it.\n",
      "loading file ./output/sentencepiece.bpe.model\n",
      "loading file None\n",
      "loading file ./output/special_tokens_map.json\n",
      "loading file ./output/tokenizer_config.json\n",
      "loading file None\n"
     ]
    }
   ],
   "source": [
    "model = MBartForConditionalGeneration.from_pretrained(\"./output\")\n",
    "tokenizer = MBartTokenizer.from_pretrained(\"./output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "日本語 - おはよう: English - it's okay. it's okay.\n"
     ]
    }
   ],
   "source": [
    "sentence = \"おはよう\"\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "translated_tokens = model.generate(**inputs, decoder_start_token_id=tokenizer.lang_code_to_id[\"en_XX\"], early_stopping=True, max_length=48)\n",
    "pred = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n",
    "print(f\"日本語 - {sentence}: English - {pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Base Python)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/python-3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
